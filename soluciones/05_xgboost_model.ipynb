{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data wrangling\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils.class_weight import compute_sample_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run \"C:/Users/mjkipsz2/OneDrive - The University of Manchester/Desktop/Pump failure/utils.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location=('file:///c:/Users/mjkipsz2/OneDrive - The University of '\n",
       " 'Manchester/Desktop/Pump failure/notebook/mlruns/307286651761719792'), creation_time=1734704895170, experiment_id='307286651761719792', last_update_time=1734704895170, lifecycle_stage='active', name='pump_failure_prediction', tags={}>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "mlflow.set_tracking_uri(\"file:./mlruns\")\n",
    "mlflow.set_experiment(\"pump_failure_prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Air_temperature</th>\n",
       "      <th>Process_temperature</th>\n",
       "      <th>Rotational_speed</th>\n",
       "      <th>Torque</th>\n",
       "      <th>Tool_wear</th>\n",
       "      <th>Type_High</th>\n",
       "      <th>Type_Low</th>\n",
       "      <th>Type_Medium</th>\n",
       "      <th>Failure_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.951417</td>\n",
       "      <td>-0.946356</td>\n",
       "      <td>0.067484</td>\n",
       "      <td>0.283054</td>\n",
       "      <td>-1.695647</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>No Failure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.901428</td>\n",
       "      <td>-0.878954</td>\n",
       "      <td>-0.729604</td>\n",
       "      <td>0.634238</td>\n",
       "      <td>-1.648511</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>No Failure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.951417</td>\n",
       "      <td>-1.013759</td>\n",
       "      <td>-0.227940</td>\n",
       "      <td>0.945286</td>\n",
       "      <td>-1.617087</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>No Failure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.901428</td>\n",
       "      <td>-0.946356</td>\n",
       "      <td>-0.590253</td>\n",
       "      <td>-0.048061</td>\n",
       "      <td>-1.585664</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>No Failure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.901428</td>\n",
       "      <td>-0.878954</td>\n",
       "      <td>-0.729604</td>\n",
       "      <td>0.002108</td>\n",
       "      <td>-1.554240</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>No Failure</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Air_temperature  Process_temperature  Rotational_speed    Torque  \\\n",
       "0        -0.951417            -0.946356          0.067484  0.283054   \n",
       "1        -0.901428            -0.878954         -0.729604  0.634238   \n",
       "2        -0.951417            -1.013759         -0.227940  0.945286   \n",
       "3        -0.901428            -0.946356         -0.590253 -0.048061   \n",
       "4        -0.901428            -0.878954         -0.729604  0.002108   \n",
       "\n",
       "   Tool_wear  Type_High  Type_Low  Type_Medium Failure_type  \n",
       "0  -1.695647        0.0       0.0          1.0   No Failure  \n",
       "1  -1.648511        0.0       1.0          0.0   No Failure  \n",
       "2  -1.617087        0.0       1.0          0.0   No Failure  \n",
       "3  -1.585664        0.0       1.0          0.0   No Failure  \n",
       "4  -1.554240        0.0       1.0          0.0   No Failure  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the configuration file\n",
    "with open('../config.yaml', 'r') as config_file:\n",
    "    config = yaml.safe_load(config_file)\n",
    "\n",
    "# Access the settings\n",
    "project_folder = config['projectFolder']\n",
    "df_path = os.path.join(project_folder, config['transformedDataFile'])\n",
    "df = pd.read_csv(df_path)\n",
    "\n",
    "# Display the first few rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features (X) and target variable (y)\n",
    "X = df.drop('Failure_type', axis=1)  # Features (all columns except 'Failure_type')\n",
    "y = df['Failure_type']  # Target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (7978, 8)\n",
      "Testing data shape: (1995, 8)\n"
     ]
    }
   ],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# Check the shape of the data\n",
    "print(f\"Training data shape: {X_train.shape}\")\n",
    "print(f\"Testing data shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode target labels as XGB requires numerical labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_test_encoded = label_encoder.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mjkipsz2\\OneDrive - The University of Manchester\\Desktop\\Pump failure\\.venv\\lib\\site-packages\\sklearn\\base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n",
      "c:\\Users\\mjkipsz2\\OneDrive - The University of Manchester\\Desktop\\Pump failure\\.venv\\lib\\site-packages\\sklearn\\utils\\_tags.py:354: FutureWarning: The SMOTE or classes from which it inherits use `_get_tags` and `_more_tags`. Please define the `__sklearn_tags__` method, or inherit from `sklearn.base.BaseEstimator` and/or other appropriate mixins such as `sklearn.base.TransformerMixin`, `sklearn.base.ClassifierMixin`, `sklearn.base.RegressorMixin`, and `sklearn.base.OutlierMixin`. From scikit-learn 1.7, not defining `__sklearn_tags__` will raise an error.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Accuracy': 0.9754385964912281,\n",
       " 'Balanced Accuracy': np.float64(0.8494708947179304),\n",
       " 'Macro Recall': 0.9754385964912281,\n",
       " 'Macro Precision': 0.989242283791671,\n",
       " 'Macro F1': 0.9815870281839686,\n",
       " 'F1 Scores per Class': array([0.89795918, 0.98748044, 0.69565217, 0.85714286, 0.11428571])}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating pipeline with xgboost classifier and smote\n",
    "xg_boost_smote = Pipeline(steps=[\n",
    "    ('smote', SMOTE(random_state=2023)),\n",
    "    ('model', XGBClassifier(random_state=2023))\n",
    "])\n",
    "\n",
    "# Fit pipeline \n",
    "xg_boost_smote.fit(X_train, y_train_encoded)\n",
    "\n",
    "# Generate Predictions using the correctly fitted pipeline\n",
    "y_pred = xg_boost_smote.predict(X_test)\n",
    "\n",
    "# Decode predictions back to original labels (optional)\n",
    "y_pred_decoded = label_encoder.inverse_transform(y_pred)\n",
    "\n",
    "# Evaluate Metrics\n",
    "metrics = get_metrics(y_test_encoded, y_pred)\n",
    "\n",
    "# View Results\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Accuracy': 0.9829573934837093,\n",
       " 'Balanced Accuracy': np.float64(0.8574281351060007),\n",
       " 'Macro Recall': 0.9829573934837093,\n",
       " 'Macro Precision': 0.9890159415390738,\n",
       " 'Macro F1': 0.9854568402093054,\n",
       " 'F1 Scores per Class': array([0.88      , 0.9914308 , 0.66666667, 0.85106383, 0.28571429])}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating model with xgboost classifier and balanced class weights\n",
    "\n",
    "# Compute sample weights for class imbalance\n",
    "weights = compute_sample_weight(class_weight='balanced', y=y_train_encoded)\n",
    "\n",
    "# Initialize XGBoost classifier\n",
    "xgb_model = XGBClassifier(random_state=2023)\n",
    "\n",
    "# Fit the model with sample weights\n",
    "xgb_model.fit(X_train, y_train_encoded, sample_weight=weights)\n",
    "\n",
    "# Generate predictions\n",
    "y_pred = xgb_model.predict(X_test)\n",
    "\n",
    "# Evaluate metrics\n",
    "metrics = get_metrics(y_test_encoded, y_pred)\n",
    "\n",
    "# View results\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the objective function for Optuna\n",
    "def objective(trial):\n",
    "    # Define the hyperparameter search space\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 50, 500),  # Number of trees\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 15),  # Maximum depth of trees\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),  # Learning rate\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),  # Subsample ratio\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),  # Feature subsampling\n",
    "        'gamma': trial.suggest_float('gamma', 0, 5),  # Minimum loss reduction\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 0, 10),  # L1 regularization\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 0, 10),  # L2 regularization\n",
    "    }\n",
    "\n",
    "    # Initialize the XGBoost classifier with the trial's parameters\n",
    "    model = XGBClassifier(random_state=2023, **params)\n",
    "\n",
    "\n",
    "    # Compute sample weights for class imbalance\n",
    "    weights = compute_sample_weight(class_weight='balanced', y=y_train_encoded)\n",
    "    \n",
    "    # Fit the model with sample weights\n",
    "    model.fit(X_train, y_train_encoded, sample_weight=weights)\n",
    "\n",
    "    # Generate predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Evaluate the model using F1 score (or any other metric)\n",
    "    f1 = f1_score(y_test_encoded, y_pred, average='weighted')  # Weighted F1 score for imbalanced data\n",
    "\n",
    "    return f1  # Optuna will maximize this score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-20 15:15:17,845] A new study created in memory with name: no-name-6143c4bd-8429-4ad2-b3d1-2f418b624abb\n",
      "[I 2024-12-20 15:15:19,039] Trial 0 finished with value: 0.9332163306517605 and parameters: {'n_estimators': 498, 'max_depth': 12, 'learning_rate': 0.017406459347193183, 'subsample': 0.9489754931155063, 'colsample_bytree': 0.7779877919310103, 'gamma': 1.091016386260999, 'reg_alpha': 7.299224651568444, 'reg_lambda': 6.720300571550903}. Best is trial 0 with value: 0.9332163306517605.\n",
      "[I 2024-12-20 15:15:19,753] Trial 1 finished with value: 0.9579416756596505 and parameters: {'n_estimators': 407, 'max_depth': 5, 'learning_rate': 0.11401789946303593, 'subsample': 0.5473246083117472, 'colsample_bytree': 0.900536565899873, 'gamma': 1.8710561070448413, 'reg_alpha': 0.6304070300250664, 'reg_lambda': 5.008072504444762}. Best is trial 1 with value: 0.9579416756596505.\n",
      "[I 2024-12-20 15:15:20,839] Trial 2 finished with value: 0.9147025708089696 and parameters: {'n_estimators': 390, 'max_depth': 9, 'learning_rate': 0.01090056265239977, 'subsample': 0.9540966771103085, 'colsample_bytree': 0.5792042608404595, 'gamma': 3.029132440379787, 'reg_alpha': 8.37787598500228, 'reg_lambda': 5.535239149045763}. Best is trial 1 with value: 0.9579416756596505.\n",
      "[I 2024-12-20 15:15:21,248] Trial 3 finished with value: 0.9296622796721332 and parameters: {'n_estimators': 262, 'max_depth': 15, 'learning_rate': 0.2596327939038653, 'subsample': 0.7372699576622811, 'colsample_bytree': 0.5949190278325107, 'gamma': 3.894069528947398, 'reg_alpha': 5.078783995204859, 'reg_lambda': 4.105456218422981}. Best is trial 1 with value: 0.9579416756596505.\n",
      "[I 2024-12-20 15:15:21,561] Trial 4 finished with value: 0.9216745848734152 and parameters: {'n_estimators': 105, 'max_depth': 10, 'learning_rate': 0.08712828059915102, 'subsample': 0.6947962664432918, 'colsample_bytree': 0.6941955377283011, 'gamma': 3.47572494653051, 'reg_alpha': 9.209645429222466, 'reg_lambda': 5.219641602043838}. Best is trial 1 with value: 0.9579416756596505.\n",
      "[I 2024-12-20 15:15:21,959] Trial 5 finished with value: 0.933818057834629 and parameters: {'n_estimators': 169, 'max_depth': 7, 'learning_rate': 0.059029124524034804, 'subsample': 0.973926947254945, 'colsample_bytree': 0.8203473474678347, 'gamma': 0.4838771504923106, 'reg_alpha': 9.3810875299484, 'reg_lambda': 2.4007530169274895}. Best is trial 1 with value: 0.9579416756596505.\n",
      "[I 2024-12-20 15:15:22,733] Trial 6 finished with value: 0.9423375328675723 and parameters: {'n_estimators': 477, 'max_depth': 12, 'learning_rate': 0.1322925891144611, 'subsample': 0.7483728425985471, 'colsample_bytree': 0.8368071776429953, 'gamma': 2.2643687509084813, 'reg_alpha': 2.1442244311798344, 'reg_lambda': 4.260285657867911}. Best is trial 1 with value: 0.9579416756596505.\n",
      "[I 2024-12-20 15:15:23,379] Trial 7 finished with value: 0.929989227629791 and parameters: {'n_estimators': 467, 'max_depth': 12, 'learning_rate': 0.07985368064155317, 'subsample': 0.8612839775805211, 'colsample_bytree': 0.8850936087395073, 'gamma': 4.983936807470732, 'reg_alpha': 4.923145617493306, 'reg_lambda': 9.958311373948224}. Best is trial 1 with value: 0.9579416756596505.\n",
      "[I 2024-12-20 15:15:24,308] Trial 8 finished with value: 0.920973843224433 and parameters: {'n_estimators': 377, 'max_depth': 10, 'learning_rate': 0.017195076913492244, 'subsample': 0.8820029405446528, 'colsample_bytree': 0.5507183369752151, 'gamma': 4.984577307877295, 'reg_alpha': 4.593622239910477, 'reg_lambda': 1.6585588176214738}. Best is trial 1 with value: 0.9579416756596505.\n",
      "[I 2024-12-20 15:15:25,101] Trial 9 finished with value: 0.9304278254126321 and parameters: {'n_estimators': 440, 'max_depth': 10, 'learning_rate': 0.21292655324845827, 'subsample': 0.8889516746077367, 'colsample_bytree': 0.9961583666723594, 'gamma': 2.902775938987024, 'reg_alpha': 9.488781714107414, 'reg_lambda': 8.343131450917816}. Best is trial 1 with value: 0.9579416756596505.\n",
      "[I 2024-12-20 15:15:25,732] Trial 10 finished with value: 0.9403141025709106 and parameters: {'n_estimators': 289, 'max_depth': 3, 'learning_rate': 0.031910977914913985, 'subsample': 0.5031620523897657, 'colsample_bytree': 0.9998502465410648, 'gamma': 1.7575614494406457, 'reg_alpha': 0.0639900280526563, 'reg_lambda': 0.6198087019640122}. Best is trial 1 with value: 0.9579416756596505.\n",
      "[I 2024-12-20 15:15:26,267] Trial 11 finished with value: 0.9567196885303841 and parameters: {'n_estimators': 331, 'max_depth': 5, 'learning_rate': 0.14007218064276897, 'subsample': 0.6040258000451173, 'colsample_bytree': 0.9097855494468422, 'gamma': 1.9072968313325032, 'reg_alpha': 0.5523732440709158, 'reg_lambda': 3.270380838389476}. Best is trial 1 with value: 0.9579416756596505.\n",
      "[I 2024-12-20 15:15:26,826] Trial 12 finished with value: 0.9611127694037038 and parameters: {'n_estimators': 312, 'max_depth': 4, 'learning_rate': 0.1414622742715242, 'subsample': 0.5478805352631755, 'colsample_bytree': 0.9095844002818785, 'gamma': 1.6663756707018242, 'reg_alpha': 0.09488436911124098, 'reg_lambda': 3.431347316606015}. Best is trial 12 with value: 0.9611127694037038.\n",
      "[I 2024-12-20 15:15:27,299] Trial 13 finished with value: 0.9408471658155019 and parameters: {'n_estimators': 187, 'max_depth': 3, 'learning_rate': 0.13970725030939193, 'subsample': 0.507939509538267, 'colsample_bytree': 0.6976065667279115, 'gamma': 1.0772986952144818, 'reg_alpha': 2.1207068509780553, 'reg_lambda': 6.5176492234813335}. Best is trial 12 with value: 0.9611127694037038.\n",
      "[I 2024-12-20 15:15:27,957] Trial 14 finished with value: 0.9586824214610581 and parameters: {'n_estimators': 240, 'max_depth': 6, 'learning_rate': 0.03841484364355507, 'subsample': 0.6163357919510377, 'colsample_bytree': 0.9221311146578874, 'gamma': 0.28306312925307786, 'reg_alpha': 2.204065405594485, 'reg_lambda': 0.03709467407822853}. Best is trial 12 with value: 0.9611127694037038.\n",
      "[I 2024-12-20 15:15:28,593] Trial 15 finished with value: 0.955425848339429 and parameters: {'n_estimators': 224, 'max_depth': 6, 'learning_rate': 0.036139557290825415, 'subsample': 0.632030720561419, 'colsample_bytree': 0.9333111001372361, 'gamma': 0.0160338666580202, 'reg_alpha': 2.5836056269132843, 'reg_lambda': 0.12436411576439776}. Best is trial 12 with value: 0.9611127694037038.\n",
      "[I 2024-12-20 15:15:29,369] Trial 16 finished with value: 0.9447808763905998 and parameters: {'n_estimators': 315, 'max_depth': 7, 'learning_rate': 0.04196320784133708, 'subsample': 0.6235343838631472, 'colsample_bytree': 0.7554429394851184, 'gamma': 1.0134922279232608, 'reg_alpha': 3.1962614715488336, 'reg_lambda': 1.7874637238594036}. Best is trial 12 with value: 0.9611127694037038.\n",
      "[I 2024-12-20 15:15:29,836] Trial 17 finished with value: 0.9269834862664176 and parameters: {'n_estimators': 135, 'max_depth': 4, 'learning_rate': 0.02342035153844336, 'subsample': 0.5737591270239145, 'colsample_bytree': 0.9466506070364186, 'gamma': 0.008942632106076598, 'reg_alpha': 1.342067820384396, 'reg_lambda': 2.6207312406479417}. Best is trial 12 with value: 0.9611127694037038.\n",
      "[I 2024-12-20 15:15:30,063] Trial 18 finished with value: 0.9312205927676042 and parameters: {'n_estimators': 64, 'max_depth': 8, 'learning_rate': 0.056400621321046014, 'subsample': 0.6834829002635336, 'colsample_bytree': 0.844894621663465, 'gamma': 1.326432862989315, 'reg_alpha': 3.5770659810366388, 'reg_lambda': 1.3320601641426746}. Best is trial 12 with value: 0.9611127694037038.\n",
      "[I 2024-12-20 15:15:30,609] Trial 19 finished with value: 0.9385265772270599 and parameters: {'n_estimators': 243, 'max_depth': 5, 'learning_rate': 0.07666299481596768, 'subsample': 0.6654445793312077, 'colsample_bytree': 0.6712547725234463, 'gamma': 0.453700684229458, 'reg_alpha': 5.979695763560386, 'reg_lambda': 3.2471311640061225}. Best is trial 12 with value: 0.9611127694037038.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'n_estimators': 312, 'max_depth': 4, 'learning_rate': 0.1414622742715242, 'subsample': 0.5478805352631755, 'colsample_bytree': 0.9095844002818785, 'gamma': 1.6663756707018242, 'reg_alpha': 0.09488436911124098, 'reg_lambda': 3.431347316606015}\n",
      "Best F1-score: 0.9611127694037038\n"
     ]
    }
   ],
   "source": [
    "# Create a study object\n",
    "study = optuna.create_study(direction='maximize')  # We want to maximize F1-score\n",
    "\n",
    "# Optimize the study\n",
    "study.optimize(objective, n_trials=20)  # Run 20 trials (you can increase this for better results)\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(\"Best hyperparameters:\", study.best_params)\n",
    "print(\"Best F1-score:\", study.best_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/12/20 15:15:40 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Accuracy': 0.9448621553884712,\n",
       " 'Balanced Accuracy': np.float64(0.8916582824290333),\n",
       " 'Macro Recall': 0.9448621553884712,\n",
       " 'Macro Precision': 0.9836493899640406,\n",
       " 'Macro F1': 0.9611127694037038}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract the best hyperparameters\n",
    "best_params = study.best_params\n",
    "\n",
    "# Train the final model with the best hyperparameters\n",
    "final_model = XGBClassifier(random_state=2023, **best_params)\n",
    "\n",
    "# Compute sample weights again\n",
    "weights = compute_sample_weight(class_weight='balanced', y=y_train_encoded)\n",
    "\n",
    "# Start an MLflow run\n",
    "with mlflow.start_run(run_name=\"xgboost_classification_run\"):\n",
    "\n",
    "    # Log the best hyperparameters\n",
    "    mlflow.log_params(best_params)\n",
    "    \n",
    "    # Fit the final model\n",
    "    final_model.fit(X_train, y_train_encoded, sample_weight=weights)\n",
    "    \n",
    "    # Generate predictions\n",
    "    y_pred = final_model.predict(X_test)\n",
    "    \n",
    "    # Evaluate metrics\n",
    "    metrics = get_metrics(y_test_encoded, y_pred)\n",
    "    \n",
    "    # Remove 'F1 Scores per Class' from metrics\n",
    "    if 'F1 Scores per Class' in metrics:\n",
    "        del metrics['F1 Scores per Class']\n",
    "    \n",
    "    # Log metrics\n",
    "    mlflow.log_metrics(metrics)\n",
    "    \n",
    "    # Optionally, log the final model\n",
    "    mlflow.sklearn.log_model(final_model, \"xg_boost_classifier\")\n",
    "\n",
    "# View results\n",
    "metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "My env pump failure)",
   "language": "python",
   "name": "my-env-with-poetry"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
